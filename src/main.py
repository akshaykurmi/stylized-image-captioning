import logging
import os

import argparse
import shutil

from .datasets import PersonalityCaptions, DatasetManager
from .train import pretrain_generator, pretrain_discriminator, adversarially_train_generator_and_discriminator
from .utils import init_logging

logger = logging.getLogger(__name__)

parser = argparse.ArgumentParser()

parser.add_argument("--run_id", required=True, type=int)

parser.add_argument("--overwrite_run_results", default=False, action="store_true")
parser.add_argument("--overwrite_cached_dataset", default=False, action="store_true")
parser.add_argument("--run_download_dataset", default=False, action="store_true")
parser.add_argument("--run_cache_dataset", default=False, action="store_true")
parser.add_argument("--run_generator_pretraining", default=False, action="store_true")
parser.add_argument("--run_discriminator_pretraining", default=False, action="store_true")
parser.add_argument("--run_adversarial_training", default=False, action="store_true")
parser.add_argument("--run_evaluation", default=False, action="store_true")

args = parser.parse_args()

args.run_id = f"run_{args.run_id}"
args.base_dir = os.path.dirname(os.path.dirname(__file__))
args.data_dir = os.path.join(args.base_dir, "personality_captions_data")
args.results_dir = os.path.join(args.base_dir, "results")
args.run_dir = os.path.join(args.results_dir, args.run_id)
args.checkpoints_dir = os.path.join(args.run_dir, "checkpoints")
args.log_dir = os.path.join(args.run_dir, "logs")

args.seed = 42
args.max_seq_len = 20

args.generator_encoder_units = 2048
args.generator_embedding_units = 512
args.generator_attention_units = 512
args.generator_lstm_units = 512
args.generator_z_units = 256
args.generator_lstm_dropout = 0.2
args.discriminator_embedding_units = 512
args.discriminator_lstm_units = 512

args.generator_pretrain_scheduled_sampling_initial_rate = 1
args.generator_pretrain_scheduled_sampling_k = 3500
args.generator_pretrain_learning_rate = 1e-4
args.generator_pretrain_grad_clipvalue = 5.
args.generator_pretrain_dsa_lambda = 0.9
args.generator_pretrain_batch_size = 64
args.generator_pretrain_epochs = 20
args.generator_pretrain_logging_steps = 1
args.generator_pretrain_validate_steps = 1000
args.generator_pretrain_checkpoint_steps = 50

args.discriminator_pretrain_learning_rate = 1e-4
args.discriminator_pretrain_grad_clipvalue = 5.
args.discriminator_pretrain_batch_size = 64
args.discriminator_pretrain_neg_sample_weight = 0.5
args.discriminator_pretrain_epochs = 10
args.discriminator_pretrain_logging_steps = 1
args.discriminator_pretrain_validate_steps = 1000
args.discriminator_pretrain_checkpoint_steps = 50

args.generator_adversarial_learning_rate = 1e-4
args.generator_adversarial_grad_clipvalue = 5.
args.generator_adversarial_logging_steps = 1
args.generator_adversarial_batch_size = 16
args.generator_adversarial_dsa_lambda = 0.9
args.discriminator_adversarial_learning_rate = 1e-4
args.discriminator_adversarial_grad_clipvalue = 5.
args.discriminator_adversarial_logging_steps = 1
args.discriminator_adversarial_batch_size = 6
args.discriminator_adversarial_neg_sample_weight = 0.5
args.adversarial_rounds = 10000
args.adversarial_validate_rounds = 200
args.adversarial_checkpoint_rounds = 5
args.adversarial_g_steps = 1
args.adversarial_d_steps = 5
args.adversarial_rollout_n = 5
args.adversarial_rollout_update_rate = 1

init_logging(args.log_dir)

personality_captions = PersonalityCaptions(args.data_dir)
dataset_loader = DatasetManager(personality_captions, args.max_seq_len)

if args.run_download_dataset:
    logger.info("***** Downloading Dataset *****")
    personality_captions.download()

if args.run_cache_dataset:
    logger.info("***** Caching dataset as TFRecords *****")
    if args.overwrite_cached_dataset:
        shutil.rmtree(args.cache_dir, ignore_errors=True)
    os.makedirs(args.cache_dir, exist_ok=False)
    dataset_loader.cache_dataset("val", batch_size=32, num_batches_per_shard=80)
    dataset_loader.cache_dataset("test", batch_size=32, num_batches_per_shard=80)
    dataset_loader.cache_dataset("train", batch_size=32, num_batches_per_shard=80)

if args.run_generator_pretraining:
    if args.overwrite_run_results:
        shutil.rmtree(args.run_dir, ignore_errors=True)
    pretrain_generator(args, dataset_loader)

if args.run_discriminator_pretraining:
    if args.overwrite_run_results:
        shutil.rmtree(args.run_dir, ignore_errors=True)
    pretrain_discriminator(args, dataset_loader)

if args.run_adversarial_training:
    if args.overwrite_run_results:
        shutil.rmtree(args.run_dir, ignore_errors=True)
    adversarially_train_generator_and_discriminator(args, dataset_loader)

if args.run_evaluation:
    pass
